{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wendy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wendy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "mypath = \"./Case Presentation 1\"\n",
    "files = os.listdir(mypath)\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def txt_to_df(files):\n",
    "    df = pd.DataFrame(columns=[\"content\", \"label\"])\n",
    "    for f in files:\n",
    "        txtFile = open('./Case Presentation 1/'+f, 'r')\n",
    "        tempTxt = txtFile.read().splitlines()\n",
    "        flag = False\n",
    "        content = \"\"\n",
    "        \n",
    "        for line in tempTxt:\n",
    "            if line[-1:] == \":\" : \n",
    "                flag = True\n",
    "                continue\n",
    "            if flag:\n",
    "                content += (\" \" + line)\n",
    "            \n",
    "            content = re.sub(r'[^A-Za-z\\s]',r' ',content)\n",
    "            # Lowercase\n",
    "            content = \" \".join([w.lower() for w in content.split()])\n",
    "            # Remove Stop\n",
    "            content = \" \".join([w for w in content.split() if w not in stop_words])\n",
    "            # Stemming\n",
    "            #st = PorterStemmer()\n",
    "            #content = \" \".join([st.stem(w) for w in content.split()])\n",
    "            # \n",
    "            content = \" \".join([lemmatizer.lemmatize(w, pos='v') for w in content.split()])\n",
    "            #content = \" \".join([lemmatizer.lemmatize(w, pos='n') for w in content.split()])\n",
    "                \n",
    "        if f[:3] == 'CUR':\n",
    "            label = 'smoker'\n",
    "        elif f[:3] == 'PAS':\n",
    "            label = 'past_smoker'\n",
    "        elif f[:3] == 'NON':\n",
    "            label = 'non_smoker'\n",
    "        else:\n",
    "            label = 'unknown'\n",
    "            \n",
    "        df = df.append({\n",
    "            \"content\": content,\n",
    "            \"label\": label,\n",
    "\n",
    "        }, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "files = os.listdir(\"./Case Presentation 1/\")\n",
    "df = txt_to_df(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = pd.DataFrame(df.iloc[:,0])\n",
    "df_y = np.array(df.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_x, df_y, test_size=8, stratify=df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need survey how does TF-IDF work, not yet done\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "corpus_train = list(X_train[\"content\"])\n",
    "vectorizer = CountVectorizer(stop_words=stop_words, analyzer='word', max_features=500)\n",
    "X = vectorizer.fit_transform(corpus_train) \n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=True)\n",
    "tfidf = transformer.fit_transform(X)\n",
    "r = pd.DataFrame(tfidf.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "corpus_val = list(X_val[\"content\"])\n",
    "X_val = vectorizer.transform(corpus_val) \n",
    "tfidf_val = transformer.transform(X_val)\n",
    "r_val = pd.DataFrame(tfidf_val.toarray(),columns=vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 20 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'kernel': 'poly'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svc = svm.SVC()\n",
    "\n",
    "grid = GridSearchCV(estimator=svc, \n",
    "                    param_grid={'C': [1, 5, 10, 50, 100], \n",
    "                                'kernel': ('linear', 'rbf', 'poly', 'sigmoid')}, cv=4, scoring='accuracy', verbose=1)\n",
    "\n",
    "grid_result = grid.fit(r, y_train)\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predict = grid_result.best_estimator_.predict(r_val)\n",
    "accuracy_score(predict, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 28 candidates, totalling 112 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 112 out of 112 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 15, 'splitter': 'random'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree = tree.DecisionTreeClassifier\n",
    "\n",
    "grid = GridSearchCV(estimator=tree(),\n",
    "             param_grid={'criterion':['gini','entropy'],\n",
    "                         'splitter':['best', 'random'],\n",
    "                         'max_depth':[1,5,10,15,20,25,30]}, cv=4, scoring='accuracy', verbose=1)\n",
    "\n",
    "grid_result = grid.fit(r, y_train)\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = grid_result.best_estimator_.predict(r_val)\n",
    "accuracy_score(predict, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 56 candidates, totalling 224 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 224 out of 224 | elapsed:   12.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 10, 'n_estimators': 100}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier\n",
    "\n",
    "grid = GridSearchCV(estimator=forest(),\n",
    "             param_grid={'n_estimators':[10,20,50,100],\n",
    "                         'max_depth':[1,5,10,15,20,25,30],\n",
    "                         'criterion':['gini', 'entropy']}, cv=4, scoring='accuracy', verbose=1)\n",
    "\n",
    "grid_result = grid.fit(r, y_train)\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = grid_result.best_estimator_.predict(r_val)\n",
    "accuracy_score(predict, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 14 candidates, totalling 56 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1000.0, 'penalty': 'l1', 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "grid = GridSearchCV(estimator=lr,\n",
    "             param_grid={'penalty':['l1', 'l2'],\n",
    "                         'C':np.logspace(-3,3,7),\n",
    "                         'solver':['liblinear']},cv=4, scoring='accuracy', verbose=1)\n",
    "\n",
    "grid_result = grid.fit(r, y_train)\n",
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = grid_result.best_estimator_.predict(r_val)\n",
    "accuracy_score(predict, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
